{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56af501a-e2a0-4c54-bc53-e3f8b3d4b36d",
   "metadata": {},
   "source": [
    "### AUtoGrade:- \n",
    "> Autograd is PyTorchâ€™s automatic differentiation engine. It automatically computes gradients (derivatives) of tensors with respect to model parameters like weights and biases, used for backpropagation during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4014ed7d-7ea4-407b-b4f8-1e346ff0737d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "583557dd-76b4-41e9-bc56-43c697419c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "139a1522-66a1-4ca1-8b46-a38a94fb09a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually find the derivative of x^2\n",
    "def dy_dx(x):\n",
    "    return 2*x\n",
    "dy_dx(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a9cff84-3cdf-4afd-8348-47cedc2f97c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3., requires_grad=True)\n",
      "tensor(9., grad_fn=<PowBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the derivative using autograd.\n",
    "x=torch.tensor(3.0, requires_grad=True) # without requires_grad=True pytorch not start backpropagation.\n",
    "y=x**2 # find the derivative of x^2\n",
    "print(x)\n",
    "print(y)\n",
    "y.backward() # it's start backpropagation using .backward.\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ba27382d-4936-4075-b899-1cec3b895e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.661275842587077"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manually find the derivative of y w.r.t x where y=x^2 and z w.r.t x where z=sin(y).\n",
    "def dz_dx(x):\n",
    "    return 2*x*math.cos(x**2)\n",
    "dz_dx(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5ae4f252-30e0-4abc-8e23-361b2a49d28e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-7.6613)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.tensor(4.0,requires_grad=True)\n",
    "y=x**2\n",
    "z=torch.sin(y)\n",
    "z.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68c61c6-16e6-4c2a-9c44-a02cf035c790",
   "metadata": {},
   "source": [
    "## Train Simple NN Manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "43511d8e-1d2b-4280-9aa8-85478b81cc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9988)\n",
      "tensor(6.7012)\n",
      "Manual Gradient of loss w.r.t weight(dw) :- 6.691762447357178\n",
      "Manual Gradient of loss w.r.t bais(db) :- 0.998770534992218\n"
     ]
    }
   ],
   "source": [
    "# Inputs\n",
    "x=torch.tensor(6.7) # Inputs Feature\n",
    "y=torch.tensor(0.0) # Target (True label) Binary classification.\n",
    "\n",
    "w=torch.tensor(1.0) # Weight\n",
    "b=torch.tensor(0.0) # Bais\n",
    "\n",
    "# Binary Cross-Entropy Loss for scalar.\n",
    "def binary_cross_entropy(prediction,target):\n",
    "    epsilon = 1e-8 # to prevent log(0)\n",
    "    prediction = torch.clamp(prediction,epsilon,1-epsilon)\n",
    "    return -(target * torch.log(prediction) + (1 - target) * torch.log(1 - prediction))\n",
    "\n",
    "# Forward  Pass\n",
    "z= w * x + b # Weighted Sum (linear Part)\n",
    "y_pred=torch.sigmoid(z) # Predicted Probability (non-linear part)\n",
    "print(y_pred)\n",
    "# print(y)\n",
    "# print(z)\n",
    "\n",
    "# Compute binary_cross_entropy loss\n",
    "loss= binary_cross_entropy(y_pred,y)\n",
    "print(loss)\n",
    "\n",
    "# Find Derivatives to update weight and bais manually\n",
    "\n",
    "# 1. dl/d(y_pred):- Loss w.r.t the prediction (y_pred)\n",
    "dloss_dy_pred=(y_pred - y) / (y_pred * (1 - y_pred))\n",
    "\n",
    "# 2. dy_pred/d_z:- y_pred w.r.t z (sigmoid derivative)\n",
    "dy_pred_dz=y_pred * (1 - y_pred)\n",
    "\n",
    "# 3. d_z/d_w: z w.r.t w and b\n",
    "dz_dw = x # dz/dw = x\n",
    "dz_db = 1 #dz/db = 1 (bais contribute direct to z)\n",
    "\n",
    "dl_dw = dloss_dy_pred * dy_pred_dz * dz_dw\n",
    "dl_db = dloss_dy_pred * dy_pred_dz * dz_db\n",
    "\n",
    "print(f\"Manual Gradient of loss w.r.t weight(dw) :- {dl_dw}\")\n",
    "print(f\"Manual Gradient of loss w.r.t bais(db) :- {dl_db}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ce9501df-99da-432d-b6a6-faefefc1ea33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.7000, grad_fn=<AddBackward0>)\n",
      "tensor(0.9988, grad_fn=<SigmoidBackward0>)\n",
      "tensor(6.7012, grad_fn=<NegBackward0>)\n",
      "tensor(6.6918)\n",
      "tensor(0.9988)\n"
     ]
    }
   ],
   "source": [
    "# Using autograd update weight and bais.\n",
    "\n",
    "x=torch.tensor(6.7)\n",
    "y=torch.tensor(0.0)\n",
    "\n",
    "w=torch.tensor(1.0, requires_grad=True)\n",
    "b=torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "z= w * x + b\n",
    "print(z)\n",
    "\n",
    "y_pred= torch.sigmoid(z)\n",
    "print(y_pred)\n",
    "\n",
    "loss=binary_cross_entropy(y_pred,y)\n",
    "print(loss)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6b9770f3-becb-4ae2-8454-e1614f43f4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], requires_grad=True)\n",
      "tensor(4.6667, grad_fn=<MeanBackward0>)\n",
      "tensor([0.6667, 1.3333, 2.0000])\n"
     ]
    }
   ],
   "source": [
    "# Working on vector. \n",
    "x=torch.tensor([1.0,2.0,3.0], requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y= (x ** 2).mean()\n",
    "print(y)\n",
    "\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "0f5274d9-d17c-46ed-b85f-99ceeea094e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2., requires_grad=True)\n",
      "tensor(4., grad_fn=<PowBackward0>)\n",
      "tensor(4.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clearing grad\n",
    "x=torch.tensor(2.0, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y= x ** 2\n",
    "print(y)\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(x.grad)\n",
    "\n",
    "x.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "b150d8bf-55ff-4818-bba2-e5f8c9171f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2., requires_grad=True)\n",
      "tensor(4., grad_fn=<PowBackward0>)\n",
      "tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "# disable gradient tracking\n",
    "# option 1 - requires_grad_(False)\n",
    "# option 2 - detach()\n",
    "# option 3 - torch.no_grad()\n",
    "\n",
    "x=torch.tensor(2.0,requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y= x ** 2\n",
    "print(y)\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "245d551a-4a5e-4616-b512-959ebc0267a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2., requires_grad=True)\n",
      "tensor(4., grad_fn=<PowBackward0>)\n",
      "tensor(4.)\n",
      "tensor(2.)\n",
      "tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "# Option 1 - requires_grad_(False)\n",
    "x=torch.tensor(2.0,requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y= x ** 2\n",
    "print(y)\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(x.grad)\n",
    "\n",
    "x.requires_grad_(False)\n",
    "print(x)\n",
    "\n",
    "y= x ** 2\n",
    "print(y)\n",
    "\n",
    "# y.backward() # Give error when run y.backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "f042cb02-11c8-4edc-9dc2-f7b9a12cf632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2., requires_grad=True)\n",
      "tensor(4., grad_fn=<PowBackward0>)\n",
      "tensor(4.)\n",
      "tensor(2.)\n",
      "tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "# option 2 - detach()\n",
    "x=torch.tensor(2.0,requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y= x ** 2\n",
    "print(y)\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(x.grad)\n",
    "\n",
    "z=x.detach()\n",
    "print(z)\n",
    "\n",
    "y1= z ** 2\n",
    "print(y1)\n",
    "\n",
    "# y1.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "4e9f9003-3c56-4fd3-9e50-b7fc9c31e7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2., requires_grad=True)\n",
      "tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "# Option 1 - requires_grad_(False)\n",
    "x=torch.tensor(2.0,requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y= x ** 2\n",
    "    print(y)\n",
    "\n",
    "# y.backward()\n",
    "\n",
    "# print(x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5060d83d-8413-458f-bc19-7f68e4d18e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
