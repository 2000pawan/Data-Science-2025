


# pip install torch torchvision torchaudio


import torch
import math


# Manually find the derivative of x^2
def dy_dx(x):
    return 2*x
dy_dx(3)


# find the derivative using autograd.
x=torch.tensor(3.0, requires_grad=True) # without requires_grad=True pytorch not start backpropagation.
y=x**2 # find the derivative of x^2
print(x)
print(y)
y.backward() # it's start backpropagation using .backward.
x.grad


# manually find the derivative of y w.r.t x where y=x^2 and z w.r.t x where z=sin(y).
def dz_dx(x):
    return 2*x*math.cos(x**2)
dz_dx(4)


x=torch.tensor(4.0,requires_grad=True)
y=x**2
z=torch.sin(y)
z.backward()
x.grad





# Inputs
x=torch.tensor(6.7) # Inputs Feature
y=torch.tensor(0.0) # Target (True label) Binary classification.

w=torch.tensor(1.0) # Weight
b=torch.tensor(0.0) # Bais

# Binary Cross-Entropy Loss for scalar.
def binary_cross_entropy(prediction,target):
    epsilon = 1e-8 # to prevent log(0)
    prediction = torch.clamp(prediction,epsilon,1-epsilon)
    return -(target * torch.log(prediction) + (1 - target) * torch.log(1 - prediction))

# Forward  Pass
z= w * x + b # Weighted Sum (linear Part)
y_pred=torch.sigmoid(z) # Predicted Probability (non-linear part)
print(y_pred)
# print(y)
# print(z)

# Compute binary_cross_entropy loss
loss= binary_cross_entropy(y_pred,y)
print(loss)

# Find Derivatives to update weight and bais manually

# 1. dl/d(y_pred):- Loss w.r.t the prediction (y_pred)
dloss_dy_pred=(y_pred - y) / (y_pred * (1 - y_pred))

# 2. dy_pred/d_z:- y_pred w.r.t z (sigmoid derivative)
dy_pred_dz=y_pred * (1 - y_pred)

# 3. d_z/d_w: z w.r.t w and b
dz_dw = x # dz/dw = x
dz_db = 1 #dz/db = 1 (bais contribute direct to z)

dl_dw = dloss_dy_pred * dy_pred_dz * dz_dw
dl_db = dloss_dy_pred * dy_pred_dz * dz_db

print(f"Manual Gradient of loss w.r.t weight(dw) :- {dl_dw}")
print(f"Manual Gradient of loss w.r.t bais(db) :- {dl_db}")


# Using autograd update weight and bais.

x=torch.tensor(6.7)
y=torch.tensor(0.0)

w=torch.tensor(1.0, requires_grad=True)
b=torch.tensor(0.0, requires_grad=True)

z= w * x + b
print(z)

y_pred= torch.sigmoid(z)
print(y_pred)

loss=binary_cross_entropy(y_pred,y)
print(loss)

loss.backward()

print(w.grad)
print(b.grad)


# Working on vector. 
x=torch.tensor([1.0,2.0,3.0], requires_grad=True)
print(x)

y= (x ** 2).mean()
print(y)

y.backward()
print(x.grad)


# Clearing grad
x=torch.tensor(2.0, requires_grad=True)
print(x)

y= x ** 2
print(y)

y.backward()

print(x.grad)

x.grad.zero_()


# disable gradient tracking
# option 1 - requires_grad_(False)
# option 2 - detach()
# option 3 - torch.no_grad()

x=torch.tensor(2.0,requires_grad=True)
print(x)

y= x ** 2
print(y)

y.backward()

print(x.grad)


# Option 1 - requires_grad_(False)
x=torch.tensor(2.0,requires_grad=True)
print(x)

y= x ** 2
print(y)

y.backward()

print(x.grad)

x.requires_grad_(False)
print(x)

y= x ** 2
print(y)

# y.backward() # Give error when run y.backward


# option 2 - detach()
x=torch.tensor(2.0,requires_grad=True)
print(x)

y= x ** 2
print(y)

y.backward()

print(x.grad)

z=x.detach()
print(z)

y1= z ** 2
print(y1)

# y1.backward()


# Option 1 - requires_grad_(False)



